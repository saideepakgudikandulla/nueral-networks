# -*- coding: utf-8 -*-
"""Intro to Neural Networks Assignment #5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xUemkiwnRqUcTalTyWno4ER9TnQfIyg8

Load the Fashion-MNIST Dataset
- Use the Fashion-MNIST dataset provided by TensorFlow/Keras. The dataset contains
grayscale images of 10 categories of clothing items, each sized 28x28 pixels.
- Normalize the Data: scale pixel values to the range [0, 1].
- Split the Dataset: Use 80% of the training data for training and 20% for validation.
- Convert grayscale images to RGB by repeating the grayscale channel across three
channels.
- Resize all images to 96x96 to match the input size required by MobileNetV2
"""

# Import necessary libraries
import numpy as np
import tensorflow as tf
from tensorflow import keras

# Load Fashion-MNIST dataset
(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()

# Normalize data
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# Split dataset
train_idx = int(0.8 * len(X_train))
X_val = X_train[train_idx:]
y_val = y_train[train_idx:]
X_train = X_train[:train_idx]
y_train = y_train[:train_idx]

# Convert grayscale images to RGB
X_train = np.repeat(X_train[..., np.newaxis], 3, axis=-1)
X_val = np.repeat(X_val[..., np.newaxis], 3, axis=-1)
X_test = np.repeat(X_test[..., np.newaxis], 3, axis=-1)

# Resize images to 96x96
X_train = tf.image.resize(X_train, (96, 96))
X_val = tf.image.resize(X_val, (96, 96))
X_test = tf.image.resize(X_test, (96, 96))

# Print shapes
print("Training data shape:", X_train.shape)
print("Validation data shape:", X_val.shape)
print("Test data shape:", X_test.shape)

"""Data Augmentation
- Use ImageDataGenerator to apply transformations like random rotations, zooming,
horizontal flips, and shifts. Ensure augmentation is applied only to the training data.
- Visualize Augmented Images: Display a batch of augmented images. Ensure proper
scaling to the range [0, 255] for visualization to avoid black images
"""

# Import necessary libraries
import numpy as np
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt

# Load Fashion-MNIST dataset
(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()

# Normalize data
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# Convert grayscale images to RGB
X_train = np.repeat(X_train[..., np.newaxis], 3, axis=-1)
X_test = np.repeat(X_test[..., np.newaxis], 3, axis=-1)

# Resize images to 96x96
X_train = tf.image.resize(X_train, (96, 96))
X_test = tf.image.resize(X_test, (96, 96))

# Data augmentation
datagen = keras.preprocessing.image.ImageDataGenerator(
    rotation_range=30,
    zoom_range=0.2,
    horizontal_flip=True,
    width_shift_range=0.1,
    height_shift_range=0.1
)

# Create augmented dataset
augmented_dataset = datagen.flow(X_train, y_train, batch_size=9)

# Visualize augmented images
plt.figure(figsize=(10, 10))
for i, (imgs, _) in enumerate(augmented_dataset):
    for j, img in enumerate(imgs):
        plt.subplot(3, 3, j+1)
        plt.imshow((img * 255).astype('uint8'))
        plt.axis('off')
    break
plt.show()

"""Transfer Learning
- Load a Pre-Trained Model: Use MobileNetV2 (pre-trained on ImageNet) without the top
classification layer (include_top=False).
- Freeze all layers in the base model.
- Add Custom Layers: Add layers for feature extraction and classification, including a global
average pooling layer, dense layers, and a final softmax output layer.
- Compile the Model: Use the Adam optimizer with sparse categorical cross-entropy loss.
- Train the Model: Train the model using augmented data and evaluate it on the validation
set.

"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.applications import MobileNetV2

# Load Fashion-MNIST dataset
(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()

# Normalize data
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# Convert grayscale images to RGB
X_train = np.repeat(X_train[..., np.newaxis], 3, axis=-1)
X_test = np.repeat(X_test[..., np.newaxis], 3, axis=-1)

# Resize images
image_size = 32
X_train_resized = tf.image.resize(X_train, (image_size, image_size))
X_test_resized = tf.image.resize(X_test, (image_size, image_size))

# Load pre-trained MobileNetV2 model
base_model = MobileNetV2(
    weights='imagenet',
    include_top=False,
    input_shape=(image_size, image_size, 3)
)

# Freeze base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers
model = keras.Sequential([
    base_model,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
batch_size = 16
history = model.fit(X_train_resized, y_train,
                    validation_data=(X_test_resized, y_test),
                    epochs=5, batch_size=batch_size)

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test_resized, y_test)
print(f'Test accuracy: {test_acc:.2f}')

# Clear session
tf.keras.backend.clear_session()

"""Fine-Tune the Model
- Unfreeze the Last Few Layers: Fine-tune the model by unfreezing the last few layers of
the base model and re-training with a lower learning rate.
- Evaluate Performance: Compare the performance of the fine-tuned model with the
earlier frozen model
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.applications import MobileNetV2

# Load Fashion-MNIST dataset
(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()

# Normalize data
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# Convert grayscale images to RGB
X_train = np.repeat(X_train[..., np.newaxis], 3, axis=-1)
X_test = np.repeat(X_test[..., np.newaxis], 3, axis=-1)

# Resize images
image_size = 32
X_train_resized = tf.image.resize(X_train, (image_size, image_size))
X_test_resized = tf.image.resize(X_test, (image_size, image_size))

# Load pre-trained MobileNetV2 model
base_model = MobileNetV2(
    weights='imagenet',
    include_top=False,
    input_shape=(image_size, image_size, 3)
)

# Unfreeze last 2 layers
for layer in base_model.layers[:-2]:
    layer.trainable = False
for layer in base_model.layers[-2:]:
    layer.trainable = True

# Add custom layers
model = keras.Sequential([
    base_model,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
batch_size = 16
history = model.fit(X_train_resized, y_train,
                    validation_data=(X_test_resized, y_test),
                    epochs=10, batch_size=batch_size)

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test_resized, y_test)
print(f'Test accuracy: {test_acc:.2f}')

"""Model Evaluation
- Test the Model: Evaluate the final model on the test set.
- Generate a confusion matrix and a classification report.
- Compare Models: Compare results of the model with and without fine-tuning.
- Analyze the impact of data augmentation.

"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.applications import MobileNetV2
from sklearn.metrics import confusion_matrix, classification_report
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Load Fashion-MNIST dataset
(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()

# Normalize data
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# Convert grayscale images to RGB
X_train = np.repeat(X_train[..., np.newaxis], 3, axis=-1)
X_test = np.repeat(X_test[..., np.newaxis], 3, axis=-1)

# Resize images
image_size = 32
X_train_resized = tf.image.resize(X_train, (image_size, image_size))
X_test_resized = tf.image.resize(X_test, (image_size, image_size))

# Data augmentation
datagen = ImageDataGenerator(
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)

# Load pre-trained MobileNetV2 model
base_model = MobileNetV2(
    weights='imagenet',
    include_top=False,
    input_shape=(image_size, image_size, 3)
)

# Unfreeze last 2 layers
for layer in base_model.layers[:-2]:
    layer.trainable = False
for layer in base_model.layers[-2:]:
    layer.trainable = True

# Add custom layers
model = keras.Sequential([
    base_model,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train with data augmentation
history = model.fit(datagen.flow(X_train_resized, y_train, batch_size=16),
                    validation_data=(X_test_resized, y_test),
                    epochs=10)

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test_resized, y_test)
print(f'Test accuracy: {test_acc:.2f}')

# Predict labels
y_pred = model.predict(X_test_resized)
y_pred_class = np.argmax(y_pred, axis=1)

# Confusion matrix
cm = confusion_matrix(y_test, y_pred_class)
print("Confusion Matrix:")
print(cm)

# Classification report
cr = classification_report(y_test, y_pred_class)
print("Classification Report:")
print(cr)